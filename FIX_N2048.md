# Fix for n=2048/4096 CUDA Kernel Launch Error

## Problem
```
CUDA error: invalid configuration argument
```
at n=2048 and n=4096 for fp32/fp16 kernels.

## Root Cause
The simplified kernels used **1 thread per element**:
- n=1024 → 1024 threads ✓ (within GPU limit)
- n=2048 → 2048 threads ✗ (exceeds 1024 thread/block limit)
- n=4096 → 4096 threads ✗ (exceeds 1024 thread/block limit)

## Solution
Modified dispatch and kernels to:
1. **Cap threads at 1024** in dispatch functions
2. **Use strided loops** in kernels to handle multiple elements per thread

### Changes Made

#### Dispatch (fwht_cuda.cu lines ~2255, ~2285)
```cpp
// Before
unsigned int threads = (unsigned int)n;  // Could exceed 1024!

// After
unsigned int threads = (n <= 1024) ? (unsigned int)n : 1024u;  // Capped
```

#### Kernels (both fp32 and fp16)
```cpp
// Before: Single element per thread
if (tid < N) {
    smem[tid] = batch_data[tid];
}

// After: Strided loop for multiple elements per thread
for (int i = tid; i < N; i += num_threads) {
    smem[i] = batch_data[i];
}
```

### Butterfly Loop
The butterfly stage loop now also uses strided access:
```cpp
for (int h = 1; h < N; h <<= 1) {
    for (int idx = tid; idx < N; idx += num_threads) {
        // Process element idx
    }
    __syncthreads();
}
```

## Impact
- **n=1024**: Still uses 1024 threads (optimal, no change)
- **n=2048**: Now uses 1024 threads, 2 elements/thread
- **n=4096**: Now uses 1024 threads, 4 elements/thread

## Testing
After rebuilding, all sizes should work:
```bash
cd /workspace/python
pip install -e . --force-reinstall
python tests/benchmark_all_precisions_fixed.py
```

Expected: All configs (n=1024/2048/4096) pass correctness and benchmark successfully.
